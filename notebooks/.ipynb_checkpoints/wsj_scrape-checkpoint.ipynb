{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "from selenium import webdriver                   \n",
    "from selenium.webdriver.common.keys import Keys   \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from datetime import date, timedelta, datetime as dt\n",
    "from bs4 import BeautifulSoup as bs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoteDriverStartService():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Set user app data to a new directory\n",
    "    options.add_argument(\"user-data-dir=C:\\\\Users\\\\Donley\\\\App Data\\\\Google\\\\Chrome\\\\Application\\\\User Data\\\\Kit\")\n",
    "    options.add_experimental_option(\"Proxy\", \"null\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"ignore-certificate-errors\"])\n",
    "    # Create a download path for external data sources as default: \n",
    "    options.add_experimental_option(\"prefs\", {\n",
    "      \"download.default_directory\": r\"C:\\Users\\Donley\\Documents\\GA_TECH\\SUBMISSIONS\\PROJECT2-CHALLENGE\\data\\external\",\n",
    "      \"download.prompt_for_download\": False,\n",
    "      \"download.directory_upgrade\": True,\n",
    "      \"safebrowsing.enabled\": True\n",
    "    }),\n",
    "    # Add those optional features to capabilities\n",
    "    caps = options.to_capabilities()  \n",
    "    def start_driver(self):\n",
    "        return webdriver.Remote(command_executor='http://127.0.0.1:4444', \n",
    "                                desired_capabilities=self.caps)\n",
    "# Set class equal to new capabilities:\n",
    "DesiredCapabilities = RemoteDriverStartService()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for scraping: \n",
    "wsj = \"https://www.wsj.com/market-data/stocks?mod=nav_top_subsection\"\n",
    "# Download data to paths, csv's, json, etc: \n",
    "    # for external data sources\n",
    "external = \"../data/external\"\n",
    "    # for processed data sources with ID's\n",
    "processed = \"../data/processed\"\n",
    "\n",
    "# Locate Driver in system\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# save the .exe file under the same directory of the web-scrape python script.\n",
    "Path = os.path.join(current_path, \"chromedriver\")\n",
    "\n",
    "# Initialize Chrome driver and start browser session controlled by automated test software under Kit profile.\n",
    "caps = webdriver.DesiredCapabilities.CHROME.copy()\n",
    "caps['acceptInsecureCerts'] = True\n",
    "driver = webdriver.Chrome(executable_path='chromedriver', options=options, desired_capabilities=caps)\n",
    "\n",
    "# Get the URL\n",
    "driver.get(wsj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Find the IDs of the items we want to scrape for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give it time to search for ID and allow the page time to load:\n",
    "timeout = 30\n",
    "try:\n",
    "    WebDriverWait(driver, timeout).until(EC.visibility_of_element_located((By.ID, \"root\")))\n",
    "except TimeoutException:\n",
    "    driver.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Techniques to make more human-like web-scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the website detects us as a web-scraper, it will cut our connection so we cannot pull more data and have to re-start our scraper. This largely impacts the efficiency of the scraper and involves a lot of manual interference. There are a few techniques we can use to make the scraper more human-like:\n",
    "# (1) Randomize the sleep time\n",
    "# This can be easily implemented as below wherever needed:\n",
    "#sleep for sometime between 5 and 8 seconds\n",
    "# time.sleep(random.uniform(5,8))\n",
    "# (2) Randomize the user agent for the web browser\n",
    "# This is also easy and can be added to the browser options as below:\n",
    "# ua = UserAgent()\n",
    "# userAgent = ua.random\n",
    "# Firefox_options = webdriver.FirefoxOptions()\n",
    "# Firefox_options.add_argument(f’user-agent={userAgent}’)\n",
    "# browser = webdriver.Firefox(executable_path = DRIVER_BIN, options=Firefox_options)\n",
    "# (3) Use dynamic proxy/IP\n",
    "# This requires more work than the above two. Usually free proxies are not stable and most of them don’t respond to requests, so we need to first a free proxy that responds to our requests. This website (also named as “url” in the script below) provides a lot of free proxies which we scrape down for our use. We will use Python BeautifulSoup package to scrape a list of proxies, and use Python requests package to test whether the proxy responds to our requests to the link.\n",
    "# def get_proxy(link):\n",
    "#     url = \"https://www.sslproxies.org/\"\n",
    "#     r = requests.get(url)\n",
    "#     soup = BeautifulSoup(r.content, 'html5lib')\n",
    "#     proxies_list = list(map(lambda x: x[0]+':'+x[1], list(zip(map(lambda x: x.text, soup.findAll('td')[::8]), map(lambda x: x.text, soup.findAll('td')[1::8])))))\n",
    "#     while 1:\n",
    "#         try:\n",
    "#             selected_ip = choice(proxies_list)\n",
    "#             proxy = {'https': selected_ip, 'http': selected_ip}\n",
    "#             headers = {'User-Agent': ua.random}\n",
    "#             print('Using proxy:{}'.format(proxy))\n",
    "#             r = requests.request('get', link, proxies=proxy, headers=headers, timeout=5)\n",
    "#             break\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#     return proxy\n",
    "# We then add the working proxy to the browser option, similar to how we added the fake user agent:\n",
    "# link = \"https://www.expedia.com\"\n",
    "# proxy = get_proxy(link)\n",
    "# Firefox_options.add_argument('--proxy-server=%s' % proxy)\n",
    "# browser = webdriver.Firefox(executable_path = DRIVER_BIN, options=Firefox_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: The full code that runs the scraper and save the data to .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Html to generate S&P 500 table: \n",
    "table = driver.find_element_by_id(\"root\").get_attribute('outerHTML')\n",
    "tables  = pd.read_html(table)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate List of tables on page:\n",
    "wsj_tables =[df for df in tables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table 5 and rename columns:\n",
    "sp500_sectors_df = wsj_tables[5]\n",
    "sp500_sectors_df.columns = [\"S&P 500 & Sectors\", \"% Change\"]\n",
    "sp500_sectors_df.reset_index(drop=True, inplace=True)\n",
    "sp500_sectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_sectors_df.to_csv(external, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_sectors_html = sp500_sectors_df.to_html()\n",
    "sp500_sectors_table = str(sp500_sectors_html)\n",
    "sp500_sectors_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index of the links needed\n",
    "# 0\tS&P 500\n",
    "# 1\tCommunication Services\n",
    "# 2\tConsumer Discretionary\n",
    "# 3\tConsumer Staples\n",
    "# 4\tEnergy\n",
    "# 5\tFinancials\n",
    "# 6\tHealth Care\n",
    "# 7\tIndustrials\n",
    "# 8\tInformation Technology\n",
    "# 9\tMaterials\n",
    "# 10\tReal Estate\n",
    "# 11\tUtilities\n",
    "sp500_links = [(driver.find_elements_by_partial_link_text(sector)) for sector in sp500_sectors_df[\"S&P 500 & Sectors\"]]\n",
    "\n",
    "# variable Links to individual sector pages: \n",
    "sp500_link = [links.get_attribute(\"href\") for links in sp500_links[0]]\n",
    "communication_link = [links.get_attribute(\"href\") for links in sp500_links[1]]\n",
    "discretionary_link = [links.get_attribute(\"href\") for links in sp500_links[2]]\n",
    "staples_link = [links.get_attribute(\"href\") for links in sp500_links[3]]\n",
    "energy_link = [links.get_attribute(\"href\") for links in sp500_links[4]]\n",
    "financials_link = [links.get_attribute(\"href\") for links in sp500_links[5]]\n",
    "health_link = [links.get_attribute(\"href\") for links in sp500_links[6]]\n",
    "industrials_link = [links.get_attribute(\"href\") for links in sp500_links[7]]\n",
    "information_Technology_link = [links.get_attribute(\"href\") for links in sp500_links[8]]\n",
    "materials_link = [links.get_attribute(\"href\") for links in sp500_links[9]]\n",
    "real_estate_link = [links.get_attribute(\"href\") for links in sp500_links[10]]\n",
    "utilities_link = [links.get_attribute(\"href\") for links in sp500_links[11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of links for sectors: \n",
    "sector_links = [sp500_link, communication_link, discretionary_link, staples_link, energy_link, financials_link, health_link, industrials_link, information_Technology_link, materials_link, real_estate_link, utilities_link]\n",
    "sector_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get to Historical Data:\n",
    "driver.get(sp500_link[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data = driver.find_elements(By.CSS_SELECTOR, 'a.moreLink')\n",
    "historical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get to historical data download page: \n",
    "historical_data[1].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todays date\n",
    "currentDate = date.today()\n",
    "today = currentDate.strftime('%m/%d/%Y')\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date 1 week ago from today\n",
    "five_days = currentDate - timedelta(days=365)\n",
    "five = five_days.strftime('%m/%d/%Y')\n",
    "five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out Date From Form\n",
    "text_area = driver.find_element(By.CSS_SELECTOR, \"#selectDateFrom\")\n",
    "text_area.send_keys(Keys.CONTROL, \"a\")  # or Keys.COMMAND on Mac\n",
    "text_area.send_keys(five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out Date to\n",
    "text_area2 = driver.find_element(By.CSS_SELECTOR, \"#selectDateTo\")\n",
    "text_area2.send_keys(Keys.CONTROL, \"a\")  # or Keys.COMMAND on Mac\n",
    "text_area2.send_keys(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "generate_data = driver.find_element(By.ID, \"datPickerButton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download as csv  \n",
    "download_sheet = driver.find_element(By.ID, \"dl_spreadsheet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_sheet.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
