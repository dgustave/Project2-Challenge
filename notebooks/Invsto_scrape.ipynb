{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver                    # Import module \n",
    "from selenium.webdriver.common.keys import Keys   # For keyboard keys \n",
    "from selenium.webdriver.chrome.service import Service # Start and stop browser service\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup as bs               # parse this html\n",
    "import time     # Waiting function for page to load\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 2: Preparation\n",
    "# Let’s use Firefox in selenium as an example. I’ve had some problems with Google Chrome when I added dynamic IP to the program so recommend using Firefox for this exercise. Download the Firefox driver based on your system on this website, and you will get an exe file (“geckodriver.exe”) after unzip.\n",
    "# Then save the .exe file under the same directory of the web-scrape python script. Then in the script, we write the following script, which will open a blank Firefox and tell you it is controlled by automated test software.\n",
    "current_path = os.getcwd()\n",
    "DRIVER_BIN = os.path.join(current_path, \"geckodriver\")\n",
    "browser = webdriver.Firefox(executable_path = DRIVER_BIN)\n",
    "# I declared the dictionary outside the function to update the dictionary:\n",
    "invsto_dict = {}\n",
    "\n",
    "# I want it to reinitialize the driver for every funtion. \n",
    "# It implicity waits 30 seconds to perform all task in the drivers session. \n",
    "def init_driver():\n",
    "    # Locate Driver in system\n",
    "    Path = \"C:\\SeleniumDrivers\\chromedriver.exe\"\n",
    "    service = Service(Path)\n",
    "    service.start()\n",
    "    driver = webdriver.Remote(service.service_url)\n",
    "    driver.implicitly_wait(30) \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 3: Find the IDs of the items we want to scrape for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 4: Techniques to make more human-like web-scrapers\n",
    "If the website detects us as a web-scraper, it will cut our connection so we cannot pull more data and have to re-start our scraper. This largely impacts the efficiency of the scraper and involves a lot of manual interference. There are a few techniques we can use to make the scraper more human-like:\n",
    "(1) Randomize the sleep time\n",
    "This can be easily implemented as below wherever needed:\n",
    "#sleep for sometime between 5 and 8 seconds\n",
    "time.sleep(random.uniform(5,8))\n",
    "(2) Randomize the user agent for the web browser\n",
    "This is also easy and can be added to the browser options as below:\n",
    "ua = UserAgent()\n",
    "userAgent = ua.random\n",
    "Firefox_options = webdriver.FirefoxOptions()\n",
    "Firefox_options.add_argument(f’user-agent={userAgent}’)\n",
    "browser = webdriver.Firefox(executable_path = DRIVER_BIN, options=Firefox_options)\n",
    "(3) Use dynamic proxy/IP\n",
    "This requires more work than the above two. Usually free proxies are not stable and most of them don’t respond to requests, so we need to first a free proxy that responds to our requests. This website (also named as “url” in the script below) provides a lot of free proxies which we scrape down for our use. We will use Python BeautifulSoup package to scrape a list of proxies, and use Python requests package to test whether the proxy responds to our requests to the link.\n",
    "def get_proxy(link):\n",
    "    url = \"https://www.sslproxies.org/\"\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, 'html5lib')\n",
    "    proxies_list = list(map(lambda x: x[0]+':'+x[1], list(zip(map(lambda x: x.text, soup.findAll('td')[::8]), map(lambda x: x.text, soup.findAll('td')[1::8])))))\n",
    "    while 1:\n",
    "        try:\n",
    "            selected_ip = choice(proxies_list)\n",
    "            proxy = {'https': selected_ip, 'http': selected_ip}\n",
    "            headers = {'User-Agent': ua.random}\n",
    "            print('Using proxy:{}'.format(proxy))\n",
    "            r = requests.request('get', link, proxies=proxy, headers=headers, timeout=5)\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return proxy\n",
    "We then add the working proxy to the browser option, similar to how we added the fake user agent:\n",
    "link = \"https://www.expedia.com\"\n",
    "proxy = get_proxy(link)\n",
    "Firefox_options.add_argument('--proxy-server=%s' % proxy)\n",
    "browser = webdriver.Firefox(executable_path = DRIVER_BIN, options=Firefox_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 5: The full code that runs the scraper and save the data to .csv files\n",
    "Three things you need to do before running the"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
