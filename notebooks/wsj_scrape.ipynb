{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "from selenium import webdriver                   \n",
    "from selenium.webdriver.common.keys import Keys   \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from datetime import date, timedelta, datetime as dt\n",
    "from bs4 import BeautifulSoup as bs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoteDriverStartService():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Set user app data to a new directory\n",
    "    options.add_argument(\"user-data-dir=C:\\\\Users\\\\Donley\\\\App Data\\\\Google\\\\Chrome\\\\Application\\\\User Data\\\\Kit\")\n",
    "    options.add_experimental_option(\"Proxy\", \"null\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"ignore-certificate-errors\"])\n",
    "    # Create a download path for external data sources as default: \n",
    "    options.add_experimental_option(\"prefs\", {\n",
    "      \"download.default_directory\": r\"C:\\Users\\Donley\\Documents\\GA_TECH\\SUBMISSIONS\\PROJECT2-CHALLENGE\\data\\external\",\n",
    "      \"download.prompt_for_download\": False,\n",
    "      \"download.directory_upgrade\": True,\n",
    "      \"safebrowsing.enabled\": True\n",
    "    }),\n",
    "    # Add those optional features to capabilities\n",
    "    caps = options.to_capabilities()  \n",
    "    def start_driver(self):\n",
    "        return webdriver.Remote(command_executor='http://127.0.0.1:4444', \n",
    "                                desired_capabilities=self.caps)\n",
    "# Set class equal to new capabilities:\n",
    "DesiredCapabilities = RemoteDriverStartService()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for scraping: \n",
    "wsj = \"https://www.wsj.com/market-data/stocks?mod=nav_top_subsection\"\n",
    "# Download data to paths, csv's, json, etc: \n",
    "    # for external data sources\n",
    "external = \"../data/external/\"\n",
    "    # for processed data sources with ID's\n",
    "processed = \"../data/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate Driver in system\n",
    "current_path = os.getcwd()\n",
    "\n",
    "# save the .exe file under the same directory of the web-scrape python script.\n",
    "Path = os.path.join(current_path, \"chromedriver\")\n",
    "\n",
    "# Initialize Chrome driver and start browser session controlled by automated test software under Kit profile.\n",
    "caps = webdriver.DesiredCapabilities.CHROME.copy()\n",
    "caps['acceptInsecureCerts'] = True\n",
    "# caps = webdriver.DesiredCapabilities.CHROME.copy()\n",
    "# caps['acceptInsecureCerts'] = True\n",
    "# driver = webdriver.Chrome(options=options, desired_capabilities=caps)\n",
    "driver = webdriver.Chrome(executable_path='chromedriver', desired_capabilities=caps)\n",
    "\n",
    "# Get the URL\n",
    "driver.get(wsj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Find the IDs of the items we want to scrape for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give it time to search for ID and allow the page time to load:\n",
    "timeout = 30\n",
    "try:\n",
    "    WebDriverWait(driver, timeout).until(EC.visibility_of_element_located((By.ID, \"root\")))\n",
    "except TimeoutException:\n",
    "    driver.quit()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Techniques to make more human-like web-scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the website detects us as a web-scraper, it will cut our connection so we cannot pull more data and have to re-start our scraper. This largely impacts the efficiency of the scraper and involves a lot of manual interference. There are a few techniques we can use to make the scraper more human-like:\n",
    "# (1) Randomize the sleep time\n",
    "# This can be easily implemented as below wherever needed:\n",
    "#sleep for sometime between 5 and 8 seconds\n",
    "# time.sleep(random.uniform(5,8))\n",
    "# (2) Randomize the user agent for the web browser\n",
    "# This is also easy and can be added to the browser options as below:\n",
    "# ua = UserAgent()\n",
    "# userAgent = ua.random\n",
    "# Firefox_options = webdriver.FirefoxOptions()\n",
    "# Firefox_options.add_argument(f’user-agent={userAgent}’)\n",
    "# browser = webdriver.Firefox(executable_path = DRIVER_BIN, options=Firefox_options)\n",
    "# (3) Use dynamic proxy/IP\n",
    "# This requires more work than the above two. Usually free proxies are not stable and most of them don’t respond to requests, so we need to first a free proxy that responds to our requests. This website (also named as “url” in the script below) provides a lot of free proxies which we scrape down for our use. We will use Python BeautifulSoup package to scrape a list of proxies, and use Python requests package to test whether the proxy responds to our requests to the link.\n",
    "# def get_proxy(link):\n",
    "#     url = \"https://www.sslproxies.org/\"\n",
    "#     r = requests.get(url)\n",
    "#     soup = BeautifulSoup(r.content, 'html5lib')\n",
    "#     proxies_list = list(map(lambda x: x[0]+':'+x[1], list(zip(map(lambda x: x.text, soup.findAll('td')[::8]), map(lambda x: x.text, soup.findAll('td')[1::8])))))\n",
    "#     while 1:\n",
    "#         try:\n",
    "#             selected_ip = choice(proxies_list)\n",
    "#             proxy = {'https': selected_ip, 'http': selected_ip}\n",
    "#             headers = {'User-Agent': ua.random}\n",
    "#             print('Using proxy:{}'.format(proxy))\n",
    "#             r = requests.request('get', link, proxies=proxy, headers=headers, timeout=5)\n",
    "#             break\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#     return proxy\n",
    "# We then add the working proxy to the browser option, similar to how we added the fake user agent:\n",
    "# link = \"https://www.expedia.com\"\n",
    "# proxy = get_proxy(link)\n",
    "# Firefox_options.add_argument('--proxy-server=%s' % proxy)\n",
    "# browser = webdriver.Firefox(executable_path = DRIVER_BIN, options=Firefox_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: The full code that runs the scraper and save the data to .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[              Unnamed: 0      Last     Chg  %Chg\n",
       " 0                   DJIA  28303.46  530.70  1.91\n",
       " 1       Nasdaq Composite  11364.60  210.00  1.88\n",
       " 2                S&P 500   3419.45   58.50  1.74\n",
       " 3  DJ Total Stock Market  35104.71  612.56  1.78\n",
       " 4           Russell 2000   1611.04   33.75  2.14\n",
       " 5         NYSE Composite  13042.33  204.45  1.59\n",
       " 6           Barron's 400    745.74   13.03  1.78\n",
       " 7        CBOE Volatility     28.06   -1.42 -4.82\n",
       " 8           DJIA Futures  28194.00  494.00  1.78\n",
       " 9        S&P 500 Futures   3407.75   54.55  1.63,\n",
       "   Unnamed: 0_level_0               NYSE             Nasdaq\n",
       "               Issues Unnamed: 1_level_1 Unnamed: 2_level_1\n",
       "            Issues At Unnamed: 1_level_2 Unnamed: 2_level_2\n",
       "         Share Volume Unnamed: 1_level_3 Unnamed: 2_level_3\n",
       " 0          Advancing               2219               2678\n",
       " 1          Declining                817                798\n",
       " 2          Unchanged                 90                 82\n",
       " 3              Total               3126               3558\n",
       " 4          New Highs                137                152\n",
       " 5           New Lows                 23                 23\n",
       " 6              Total         3953106860         3985422678\n",
       " 7          Advancing         3317130254         3293735536\n",
       " 8          Declining          613711946          660994651\n",
       " 9          Unchanged           22264660           30692491,\n",
       "                              Unnamed: 0  Volume   Last   Chg  % Chg\n",
       " 0             CVD Equipment Corp. (CVV)   44.3M   5.74  2.76  92.62\n",
       " 1                  Sunworks Inc. (SUNW)   87.8M   4.06  1.26  45.00\n",
       " 2         Peck Co. Holdings Inc. (PECK)   24.8M  10.50  3.24  44.63\n",
       " 3   Pioneer Power Solutions Inc. (PPSI)   42.2M   6.89  2.09  43.54\n",
       " 4  Ocean Power Technologies Inc. (OPTT)  165.0M   2.98  0.80  36.70,\n",
       "                                          New Highs  Unnamed: 1\n",
       " 0                   Alibaba Group Holding Ltd. ADR      299.00\n",
       " 1  Taiwan Semiconductor Manufacturing Co. Ltd. ADR       86.79\n",
       " 2                          UnitedHealth Group Inc.      324.57\n",
       " 3                    Thermo Fisher Scientific Inc.      452.68\n",
       " 4                                    Danaher Corp.      220.81,\n",
       "                          New Lows  Unnamed: 1\n",
       " 0  Guangshen Railway Co. Ltd. ADR        8.21\n",
       " 1  Phoenix Tree Holdings Ltd. ADR        3.50\n",
       " 2            Aurora Cannabis Inc.        4.41\n",
       " 3            Peabody Energy Corp.        2.10\n",
       " 4        CONSOL Coal Resources LP        2.96,\n",
       "                 Unnamed: 0  % Chg\n",
       " 0                  S&P 500   1.74\n",
       " 1   Communication Services   0.88\n",
       " 2   Consumer Discretionary   2.47\n",
       " 3         Consumer Staples   1.19\n",
       " 4                   Energy   1.45\n",
       " 5               Financials   1.74\n",
       " 6              Health Care   1.89\n",
       " 7              Industrials   2.22\n",
       " 8   Information Technology   1.89\n",
       " 9                Materials   2.62\n",
       " 10             Real Estate   0.28\n",
       " 11               Utilities   0.63]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Html to generate S&P 500 table: \n",
    "table = driver.find_element_by_id(\"root\").get_attribute('outerHTML')\n",
    "tables  = pd.read_html(table)\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate List of tables on page:\n",
    "wsj_tables =[df for df in tables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S&amp;P 500 &amp; Sectors</th>\n",
       "      <th>% Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S&amp;P 500</td>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Communication Services</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>2.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Consumer Staples</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Energy</td>\n",
       "      <td>1.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Financials</td>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Health Care</td>\n",
       "      <td>1.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Industrials</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Information Technology</td>\n",
       "      <td>1.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Materials</td>\n",
       "      <td>2.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Real Estate</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Utilities</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         S&P 500 & Sectors  % Change\n",
       "0                  S&P 500      1.74\n",
       "1   Communication Services      0.88\n",
       "2   Consumer Discretionary      2.47\n",
       "3         Consumer Staples      1.19\n",
       "4                   Energy      1.45\n",
       "5               Financials      1.74\n",
       "6              Health Care      1.89\n",
       "7              Industrials      2.22\n",
       "8   Information Technology      1.89\n",
       "9                Materials      2.62\n",
       "10             Real Estate      0.28\n",
       "11               Utilities      0.63"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get table 5 and rename columns:\n",
    "sp500_sectors_df = wsj_tables[5]\n",
    "sp500_sectors_df.columns = [\"S&P 500 & Sectors\", \"% Change\"]\n",
    "sp500_sectors_df.reset_index(drop=True, inplace=True)\n",
    "sp500_sectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '../data/external'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-db40d92babf1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msp500_sectors_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexternal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3203\u001b[0m         )\n\u001b[1;32m-> 3204\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 184\u001b[1;33m             f, handles = get_handle(\n\u001b[0m\u001b[0;32m    185\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '../data/external'"
     ]
    }
   ],
   "source": [
    "sp500_sectors_df.to_csv(external+\"\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_sectors_html = sp500_sectors_df.to_html()\n",
    "sp500_sectors_table = str(sp500_sectors_html)\n",
    "sp500_sectors_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The index of the links needed\n",
    "# 0\tS&P 500\n",
    "# 1\tCommunication Services\n",
    "# 2\tConsumer Discretionary\n",
    "# 3\tConsumer Staples\n",
    "# 4\tEnergy\n",
    "# 5\tFinancials\n",
    "# 6\tHealth Care\n",
    "# 7\tIndustrials\n",
    "# 8\tInformation Technology\n",
    "# 9\tMaterials\n",
    "# 10\tReal Estate\n",
    "# 11\tUtilities\n",
    "sp500_links = [(driver.find_elements_by_partial_link_text(sector)) for sector in sp500_sectors_df[\"S&P 500 & Sectors\"]]\n",
    "\n",
    "# variable Links to individual sector pages: \n",
    "sp500_link = [links.get_attribute(\"href\") for links in sp500_links[0]]\n",
    "communication_link = [links.get_attribute(\"href\") for links in sp500_links[1]]\n",
    "discretionary_link = [links.get_attribute(\"href\") for links in sp500_links[2]]\n",
    "staples_link = [links.get_attribute(\"href\") for links in sp500_links[3]]\n",
    "energy_link = [links.get_attribute(\"href\") for links in sp500_links[4]]\n",
    "financials_link = [links.get_attribute(\"href\") for links in sp500_links[5]]\n",
    "health_link = [links.get_attribute(\"href\") for links in sp500_links[6]]\n",
    "industrials_link = [links.get_attribute(\"href\") for links in sp500_links[7]]\n",
    "information_Technology_link = [links.get_attribute(\"href\") for links in sp500_links[8]]\n",
    "materials_link = [links.get_attribute(\"href\") for links in sp500_links[9]]\n",
    "real_estate_link = [links.get_attribute(\"href\") for links in sp500_links[10]]\n",
    "utilities_link = [links.get_attribute(\"href\") for links in sp500_links[11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of links for sectors: \n",
    "sector_links = [sp500_link, communication_link, discretionary_link, staples_link, energy_link, financials_link, health_link, industrials_link, information_Technology_link, materials_link, real_estate_link, utilities_link]\n",
    "sector_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get to Historical Data:\n",
    "driver.get(sp500_link[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data = driver.find_elements(By.CSS_SELECTOR, 'a.moreLink')\n",
    "historical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get to historical data download page: \n",
    "historical_data[1].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todays date\n",
    "currentDate = date.today()\n",
    "today = currentDate.strftime('%m/%d/%Y')\n",
    "today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date 1 week ago from today\n",
    "five_days = currentDate - timedelta(days=365)\n",
    "five = five_days.strftime('%m/%d/%Y')\n",
    "five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out Date From Form\n",
    "text_area = driver.find_element(By.CSS_SELECTOR, \"#selectDateFrom\")\n",
    "text_area.send_keys(Keys.CONTROL, \"a\")  # or Keys.COMMAND on Mac\n",
    "text_area.send_keys(five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill out Date to\n",
    "text_area2 = driver.find_element(By.CSS_SELECTOR, \"#selectDateTo\")\n",
    "text_area2.send_keys(Keys.CONTROL, \"a\")  # or Keys.COMMAND on Mac\n",
    "text_area2.send_keys(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "generate_data = driver.find_element(By.ID, \"datPickerButton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download as csv  \n",
    "download_sheet = driver.find_element(By.ID, \"dl_spreadsheet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_sheet.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
